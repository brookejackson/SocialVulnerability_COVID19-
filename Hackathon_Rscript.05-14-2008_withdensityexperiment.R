########################################################################################
# Exploratory Data Analysis: COVID-19 Fatality Through the Lens of Social Vulnerability
# UCLA Hackathon, Spring 2020.
# EDA by Marisa Prietto, Brooke Jackson, Selina Botescu. 
# Updated 05/13/20 @ 4:40 by Selina
#########################################################################################

# If we have time to add the nursing home data, it can serve as a proxy as older people in 
# Meatpacking employees. Percentage of workers. 
# For plotting the curved line look at the zillow for the smooth (around lines 265) x = density, y=death rate (remember from W01a_spurious, y3)

setwd("/Users/brookejackson/Dropbox/Data Science Hackathon")
library(dplyr)
library(readxl)
library(data.table)
library(tidyverse)
library(lubridate)
library(corrplot)
library(ggplot2) 
library(leaps)
library(writexl)

# Read in data from John's Hopkins covid fatality table (ONLY adjusted 5-digit FIPS), Social Vulnerability Index (ONLY adjusted 5-digit FIPS).
SVI = read_excel("SVI_final.xlsx") 
covidDeaths = read_excel("covid_final.xlsx")

# Select variables for our analysis, including both SVI INDICES and SVI INDIVIDUAL variables. 
SVI = SVI[,c("FIPS", "COUNTY", "ST_ABBR","AREA_SQMI","E_TOTPOP","EP_POV", "EP_UNEMP", "EP_PCI", "EP_NOHSDP","EP_AGE65",
             "EP_AGE17","EP_DISABL","EP_SNGPNT","EP_MINRTY","EP_LIMENG","EP_MUNIT","EP_MOBILE","EP_CROWD", "EP_NOVEH","EP_GROUPQ","RPL_THEME1", 
             "RPL_THEME2", "RPL_THEME3", "RPL_THEME4","EP_UNINSUR")]
colnames(SVI)
colnames(SVI)=c("fips", "County", "State", "SquareMiles","TotalPopulation", "Poverty","Unemployed","PerCapitaIncome","NoHSDiploma","AGE65",
                "AGE17","Disabled","SingleParent","Minority","LimitedEnglish","MultiUnitHousing10","MobileHome","CrowdedRooms","NoVehicle",
                "InstGroup","Index_Socioeconomic", "Index_Household_Composition", "Index_MinorityStatus","Index_Housing_Transpo","Uninsured")

# Take a look at the data.
head(SVI)
str(SVI)

# Change Total Population to numeric 
SVI$TotalPopulation <- as.numeric(SVI$TotalPopulation)

# Change State_Dummy into factor
SVI$State <- as.factor(SVI$State)

# Remove row 1 Rio Arriba due to missing values (-999). 
SVI = SVI[-1,]

# Select the most recent date range from covid deaths data frame.
colnames(covidDeaths)
covidDeaths <- covidDeaths[,c(5,115)]
colnames(covidDeaths)=c("fips","deaths")
head(covidDeaths)

# Merge "SVI" and "covidDeaths".
newData <- merge(SVI, covidDeaths,by="fips")
head(newData)

CountyData <- newData
CountyData$density <- CountyData$TotalPopulation/CountyData$SquareMiles
CountyData$deaths_100000 = ((100000*CountyData$deaths)/CountyData$TotalPopulation)
write_xlsx(CountyData,"CountyData.xlsx")



# Remove County column
newData[ ,c("County")] <- NULL  

# Create new variable with deaths per 100,000, using covid deaths and population.
newData$deaths_100000 = ((100000*newData$deaths)/newData$TotalPopulation)

# Create density variable, using square miles and population
newData$density = newData$TotalPopulation/newData$SquareMiles


# Take a look at the data again after changes. 
head(newData)
str(newData)
colnames(newData)

####################### Correlation Plots ###############################

#Plot 1a: Y= deaths/100K, x = Population, Square Miles, SVI Theme and Individual variables, Deaths, Density
cor_tmp <- newData[, c(3:25,27,26)]
corrplot(cor(cor_tmp, use="complete.obs"), type="lower", method="number")

#Plot 1b: Y= deaths/100K, x = Population, Square Miles, SVI Theme variables, Deaths, Density
cor_index <- newData[,c(3,4,20:23,25,27,26)]
corrplot(cor(cor_index, use="complete.obs"), type="lower", method="number")

#Plot 1c: Y= deaths/100K, x = Population, Square Miles, SVI Individual variables, Deaths, Density
cor_variable <-newData[,c(3:19,24,25,27,26)]
corrplot(cor(cor_variable, use="complete.obs"), type="lower", method="number")
# There appear to be no variables that are too correlated, so we can continue without removing any variables. 

##################### Regression Analysis ###############################

# Regression 1a: Y = Deaths/100K, X = Density and SVI indices
# Since Square Miles and Population were used to calculate Density, we will not consider these variables in the regression analyses. 
# Since Deaths and Population were used to calculate Deaths/100K, we will not consider these variables in the regression analyses. 
fit1a <- lm(deaths_100000~density+Index_Socioeconomic+Index_Household_Composition+Index_MinorityStatus+Index_Housing_Transpo,data=newData)   
summary(fit1a)
# Regression 1a shows that Density is highly significant (***) and Housing & Transportation marginally significant (.) 
# Adjusted R Squared is 0.3808

#Regression 1b: Y = Deaths/100K X = Density and SVI individual variables
fit1b <- lm(deaths_100000~density+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
            MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData)   
summary(fit1b)
# Density, Poverty, PerCapitaIncome, LimitedEnglish, MultiUnitHousing and CrowdedRooms variables are highly significant (***). 
# We see NoHSDiploma, SingleParent, Minority as significant (**), InstGroup significant with confidence interval of .95 (*). 
# MobileHome and Age17 as marginally significant (.).
# NOTE: Regression 1b shows higher r squared of 0.4139, so we move forward with SVI individual variables (not SVI themes) 

#Regression 1c: Y = Deaths/100K X = Density, Density^2 and SVI individual variables
fit1c <- lm(deaths_100000~density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
              MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData)   
summary(fit1c)
# Density, Density^2, Poverty, PerCapitaIncome, NoHSDiploma, SingleParent, Minority, CrowdedRooms are highly significant (***). 
# We see Disabled and NoVehicle as significant (**), Uninsured as marginally significant (.).
# Regression 1c shows the highest r squared of 0.5818, so we keep Desnity^2.


##################### Regression Analysis with State as Dummy variable ###############################

# Next, let's look at the differences in States and how that may affect the fatality rate per 100K
# We will use the regression fit1c as our basic formula and add the "State" variable.
fit2c <- lm(deaths_100000~State+density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
              MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData) 
summary(fit2c)
# In Regression 2c, in addition to the States above, we see Density, Density^2, PerCapitaIncome, Minority highly significant ***
# MultiUnitHousing significant **, Poverty and CrowdedRooms reasonably significant * Age65 and LimitedEnglish marginally significant (.).  
# Adjusted R squared is the highest of all at 0.6601.

# What are the correlations we identified?
# Highly Significant (***):
# Connecticut     = increase  
# Georgia         = increase
# Indiana         = increase
# Louisiana       = increase
# Massachusetts   = increase
# Michigan        = increase
# New Jersey      = increase
# New York        = increase
# Ohio            = increase
# Philadelphia    = increase


#################### Plotting the Data ##################################

####################NEEDS UPDATE#########################################



# Regression 1b: Plotting the correlations that appear most significant in Regression 1c
# Deaths/100K vs Density
plot(newData$density,newData$deaths_100000, pch=19, col="blue",
     xlab="Density", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~density, data=newData), col="red", lwd=3) 

# Deaths/100K vs Poverty
plot(newData$Poverty,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,60),
     xlab="Poverty", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~Poverty, data=newData), col="red", lwd=3) 

# Deaths/100K vs Per Capita Income
plot(newData$PerCapitaIncome,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,100000),
     xlab="Per Capita Income", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~PerCapitaIncome, data=newData), col="red", lwd=3) 

# Deaths/100K vs Minority
plot(newData$Minority,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,150),
     xlab="Minority", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~Minority, data=newData), col="red", lwd=3) 

# Deaths/100K vs LimitedEnglish
plot(newData$LimitedEnglish,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,40),
     xlab="Limited English", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~LimitedEnglish, data=newData), col="red", lwd=3) 

# Deaths/100K vs NoVehicle
plot(newData$NoVehicle,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,40),
     xlab="NoVehicle", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~NoVehicle, data=newData), col="red", lwd=3) 


# Deaths/100K vs Density squared *******************************************************

# Create a new variable for Density^2 that we can plot
newData$densitysq = newData$density^2
str(newData)
summary(newData$densitysq)

CountyData$densitysq = CountyData$density^2
str(CountyData)
summary(CountyData$densitysq)
plot(CountyData$density,CountyData$deaths_100000, pch=19, col="blue",
     xlab="Density", ylab="Deaths per 100,000 people") 


#Density cubed
newData$densitycubed= newData$density^3
str(newData)
summary(newData$densitycubed)

#Density 
newData$logdensity= log(newData$density)
str(newData)
summary(newData$logdensity)

#log of Density plot
plot(newData$logdensity,newData$deaths_100000, pch=19, col="blue",,
     xlab="Density (Log)", ylab="Deaths per 100,000 people")
abline(lm(deaths_100000~logdensity, data=newData), col="red", lwd=3)

#density^2
plot(newData$densitysq,newData$deaths_100000, pch=19, col="blue",,
     xlab="Density (Squared)", ylab="Deaths per 100,000 people") 
loess_fit <- loess(deaths_100000 ~ densitysq, data=newData)
lines(newData$densitysq, predict(loess_fit), col = "red")

#density^3
plot(newData$densitycubed,newData$deaths_100000, pch=19, col="blue", ylim=c(0,100), xlim=c(0,100000000),
     xlab="Density (Cubed)", ylab="Deaths per 100,000 people") 

plot(newData$densitysq,newData$deaths_100000, pch=19, col="blue",
     xlab="Density (Squared)", ylab="Deaths per 100,000 people") 
abline(lm(deaths_100000~densitysq, data=newData), col="red", lwd=3) 
#Where are the other two outliers

#We didn't get anything meaningful from the squared,cubed and log of density
# However, when plotting Residuals, we see the signficance of the outliers
fit1d <- lm(deaths_100000~density,data=newData)
crplots(fit1d) 
par(mfrow=c(2,2))
plot(fit1d)

#This shows us significant outliers: 
newData[1858,1] #New York, New York County ()
newData[1830,1] #Bronx New York
newData[1851,1] #Kings, New York

#*****************************************************************
#Does the model still hold up without these outliers?
#Let's remove them and see: 

df.nooutliers <- newData[-c(1830,1851),]

fit1n <- lm(deaths_100000~density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
                    MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=df.nooutliers)   
summary(fit1n)



#nls_fit <- nls(deaths_100000 ~ density + densitysq, data=newData, start=list(a=))
#lines(newData$deaths_100000, predict(nls_fit), col = "red")

#**********************************************************

########## Model Selection and Cross-Validation ############################

######## Based on Model 22c WITH State variable. 
# Find the best model using "regsubset", based on Regression 2c.
# This is the model WITH the "State" variable. 
reg2c=regsubsets(deaths_100000~State+density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
                         MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData, really.big=T) 

reg2c.summary=summary(reg2c)

reg2c.summary$rsq
reg2c.summary$adjr2
which.max(reg2c.summary$adjr2)
which.min(reg2c.summary$cp)
which.min(reg2c.summary$bic)
coef(reg2c,8)

# Alternative method for this regsubset provides us with plots.  
reg2c_alternative=regsubsets(deaths_100000~State+density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
                         MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData, really.big=T) 
summary(reg2c_alternative)
reg.summary2c=summary(reg2c_alternative)
names(reg.summary2c)

reg.summary2c$rsq

par(mfrow=c(2,2))
plot(reg.summary2c$rss,xlab="Number of Variables",ylab="RSS",type="l")
points(8,reg.summary2c$rss[8], col="red",cex=2,pch=20)
plot(reg.summary2c$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")  #Maximum Adj R-Squared is around 12

which.max(reg.summary2c$adjr2)  # identify the location of the maximum point of a vector
points(8,reg.summary2c$adjr2[8], col="red",cex=2,pch=20)
plot(reg.summary2c$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary2c$cp) # Cp is AIC
points(8,reg.summary2c$cp[8],col="red",cex=2,pch=20)
which.min(reg.summary2c$bic)
plot(reg.summary2c$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(8,reg.summary2c$bic[8],col="red",cex=2,pch=20)

# This model shows that we should include 8 variables in our model, based on adjR, AIC-CP and BIC
# (Intercept)         StateGA         StateLA         StateMA         StateNJ         density    I(density^2) PerCapitaIncome        Minority 
# -9.726804e+00    1.375667e+01    2.712945e+01    3.745853e+01    6.379883e+01   -4.621646e-03    2.645160e-07    4.367365e-04    1.495972e-01 

######## Based on Model 1c WITHOUT State variable. 
# Find the best model using "regsubset", based on Regression 1c.
# This is the formula WITHOUT the "State" variable, because it turns out that the best model includes MANY State variables.
# Since our reserach focuses specifically on social vulnerability factors, we take "State" out, and mention it in oour discussion. 
reg1c=regsubsets(deaths_100000~density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
                         MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData, really.big=T) 

reg1c.summary=summary(reg1c)

reg1c.summary$rsq
reg1c.summary$adjr2
which.max(reg1c.summary$adjr2)
which.min(reg1c.summary$cp)
which.min(reg1c.summary$bic)
coef(reg1c,8)

# Alternative method for this regsubset provides us with plots. 
reg1c_alternative=regsubsets(deaths_100000~density+I(density^2)+Poverty+Unemployed+PerCapitaIncome+NoHSDiploma+AGE65+AGE17+Disabled+SingleParent+Minority+LimitedEnglish+MultiUnitHousing10+
                                     MobileHome+CrowdedRooms+NoVehicle+InstGroup+Uninsured,data=newData) 
summary(reg1c_alternative)
reg.summary1c=summary(reg1c_alternative)
names(reg.summary1c)

reg.summary1c$rsq

par(mfrow=c(2,2))
plot(reg.summary1c$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary1c$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")  #Maximum Adj R-Squared is around 12

which.max(reg.summary1c$adjr2)  # identify the location of the maximum point of a vector
points(8,reg.summary1c$adjr2[8], col="red",cex=2,pch=20)
plot(reg.summary1c$cp,xlab="Number of Variables",ylab="Cp",type='l')
which.min(reg.summary1c$cp) # Cp is AIC
points(8,reg.summary1c$cp[8],col="red",cex=2,pch=20)
which.min(reg.summary1c$bic)
plot(reg.summary1c$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(8,reg.summary1c$bic[8],col="red",cex=2,pch=20)

# This model shows that we should include 8 variables in our model, based on adjR, AIC-CP and BIC
# (Intercept)         density    I(density^2)         Poverty PerCapitaIncome    SingleParent        Minority    CrowdedRooms       NoVehicle 
# -3.340165e+01   -4.809583e-03    2.576131e-07    4.408235e-01    9.258564e-04    6.593033e-01    1.878638e-01   -1.441310e+00    4.091276e-01 

################################# FINAL MODEL #########################################################
# FINAL REGRESSION: Y = Deaths/100K X = Density, Density^2 and SVI individual variables
fit_final <- lm(deaths_100000~density+I(density^2)+Poverty+PerCapitaIncome+SingleParent+Minority+CrowdedRooms+NoVehicle,data=newData)   
summary(fit_final)

# The final model presents us with an adjusted R-squared of 0.5796
